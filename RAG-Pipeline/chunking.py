import os
from typing import List, Optional
from docling.chunking import HybridChunker
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer

model_id = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
MAX_TOKENS = tokenizer.model_max_length

def chunking(document, tokenizer, max_tokens) -> List:
    """
    ğŸ“¦ Splits a document into manageable chunks using HybridChunker.

    Args:
        document: ğŸ“„ The document object to be chunked.
        tokenizer: ğŸ”¤ Tokenizer used to tokenize text.
        max_tokens (int): ğŸ”¢ Maximum number of tokens allowed per chunk.

    Returns:
        List of chunks ğŸ§© if successful, otherwise an empty list.
    """

    # ğŸ›¡ï¸ Check if a valid document is provided
    if document is None:
        print("âŒ [Error] No document provided.")
        return []

    # ğŸ›¡ï¸ Validate tokenizer
    if tokenizer is None:
        print("âŒ [Error] Tokenizer is missing.")
        return []

    # ğŸ›¡ï¸ Validate max_tokens
    if not isinstance(max_tokens, int) or max_tokens <= 0:
        print("âš ï¸ [Warning] max_tokens should be a positive integer.")
        return []

    try:
        # ğŸ§  Initialize the HybridChunker
        print("ğŸ§  Initializing HybridChunker...")
        chunker = HybridChunker(
            tokenizer=tokenizer,
            max_tokens=max_tokens,
            merge_peers=True,
        )

        # âœ‚ï¸ Begin chunking
        print("ğŸ”§ Chunking in progress...")
        chunk_iter = chunker.chunk(dl_doc=document)
        chunks = list(chunk_iter)

        # âœ… Done
        print(f"âœ… Chunking complete. Total chunks created: {len(chunks)}")

        return chunks

    except Exception as e:
        print(f"ğŸ’¥ [Error] Chunking failed: {e}")
        return []
